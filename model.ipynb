{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import math\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 35660), ('la', 11039), ('en', 9767), ('el', 9246), ('y', 8823)]\n"
     ]
    }
   ],
   "source": [
    "word_vocab = Counter()\n",
    "char_vocab = Counter()\n",
    "char_vocab.update(['{', '}'])\n",
    "text_location = os.path.join(os.getcwd(), 'corpus/')\n",
    "filenames = os.listdir(text_location)\n",
    "for filename in filenames:\n",
    "    filename = os.path.join(text_location, filename)\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        line = f.read()\n",
    "        word_vocab.update(line.lower().split())\n",
    "        char_vocab.update(line)\n",
    "print(word_vocab.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +1 as 0 is the PAD\n",
    "char_to_index = {e:n+1 for n, e in enumerate(char_vocab)}\n",
    "index_to_char = {n+1:e for n, e in enumerate(char_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 3\n",
    "num_total_words = sum([num for word, num in word_vocab.items()])\n",
    "unigram_table = []\n",
    "Z = 0.001\n",
    "for word in word_vocab:\n",
    "    unigram_table.extend([word] * int(((word_vocab[word]/num_total_words)**0.75)/Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative(word):\n",
    "    neg = random.choice(unigram_table)\n",
    "    while neg == word.lower():\n",
    "        neg = random.choice(unigram_table)\n",
    "    return neg\n",
    "\n",
    "def prepare_files(filenames):\n",
    "    MIN_COUNT = 2\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                max_j = len(words)\n",
    "                for i, word in enumerate(words):\n",
    "                    if word_vocab[word.lower()] <= MIN_COUNT:\n",
    "                        continue\n",
    "                    frequency = word_vocab[word.lower()] / num_total_words\n",
    "                    number = 1 - math.sqrt(0.00005/frequency)\n",
    "                    if random.uniform(0, 1) <= number:\n",
    "                        continue\n",
    "                    for j in range(i - WINDOW, i + WINDOW):\n",
    "                        if (i == j) or (j < 0) or (j >= max_j):\n",
    "                            continue\n",
    "                        target = words[j]\n",
    "                        negati = get_negative(word)\n",
    "                        yield (word, target, negati)\n",
    "\n",
    "def prepare_word(word, char_to_index):\n",
    "    start = [char_to_index['{']]\n",
    "    finish = [char_to_index['}']]\n",
    "    return start + [char_to_index[char] for char in word] + finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "def get_buffer(filenames, buffer_size):\n",
    "    random.shuffle(filenames)\n",
    "    buffer = []\n",
    "    for word, target, negati in prepare_files(filenames):\n",
    "        word = prepare_word(word, char_to_index)\n",
    "        target = prepare_word(target, char_to_index)\n",
    "        negati = prepare_word(negati, char_to_index)\n",
    "        buffer.append([word, target, 1])\n",
    "        buffer.append([word, negati, 0])\n",
    "        if len(buffer) == buffer_size:\n",
    "            yield buffer\n",
    "            buffer = []\n",
    "    yield buffer\n",
    "    \n",
    "def get_batch(filenames, buffer_size, batch_size):\n",
    "    for buffer in get_buffer(filenames, buffer_size):\n",
    "        random.shuffle(buffer)\n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < len(buffer):\n",
    "            batch = buffer[sindex:eindex]\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            yield batch\n",
    "        if eindex >= len(buffer):\n",
    "            batch = buffer[sindex:]\n",
    "            yield batch\n",
    "            \n",
    "def pad_to_batch(batch):\n",
    "    sources, targets, y = zip(*batch)\n",
    "    max_sources = max([len(w) for w in sources])\n",
    "    max_targets = max([len(w) for w in targets])\n",
    "    max_length = max([max_sources, max_targets])\n",
    "    x_p, y_p = [], []\n",
    "    for i in range(len(batch)):\n",
    "        source = sources[i]\n",
    "        target = targets[i]\n",
    "        source = source + [0] * (max_length - len(source))\n",
    "        target = target + [0] * (max_length - len(target))\n",
    "        x_p.append(Variable(LongTensor(source + target)).view(1, -1))\n",
    "        y_p.append(Variable(LongTensor([y[i]])))\n",
    "    return torch.cat(x_p), torch.cat(y_p).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_location = os.path.join(os.getcwd(), 'corpus/')\n",
    "filenames = [os.path.join(text_location, filename) for filename in os.listdir(text_location)]\n",
    "batches = get_batch(filenames, BUFFER_SIZE, 100)\n",
    "hey=pad_to_batch(next(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hey[1].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding dimension 15\n",
    "# filter widths = [1, 2, 3, 4, 5, 6, 7]\n",
    "# filter dimens = [50, 100, 150, 200, 200, 200, 200]\n",
    "# tanh\n",
    "# highway network num 2\n",
    "# relu activation\n",
    "\n",
    "class Word2CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, kernel_dims, kernel_sizes, \n",
    "                 dropout=0.5, highway_layers=2):\n",
    "        super(Word2CNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim, (size, embedding_dim)) for dim, size in zip(kernel_dims, kernel_sizes)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        internal_dim = sum(kernel_dims)\n",
    "        self.hw_num_layers = highway_layers\n",
    "        self.hw_nonlinear = nn.ModuleList([nn.Linear(internal_dim, internal_dim) for _ in range(highway_layers)])\n",
    "        self.hw_linear = nn.ModuleList([nn.Linear(internal_dim, internal_dim) for _ in range(highway_layers)])\n",
    "        self.hw_gate = nn.ModuleList([nn.Linear(internal_dim, internal_dim) for _ in range(highway_layers)])\n",
    "        self.final_layer = nn.Linear(internal_dim * 2, 2)\n",
    "        \n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = inputs.view(inputs.size()[0]*2, -1) # each word on a line [B, MAX_LENGTH]\n",
    "        inputs = self.embeddings(inputs).unsqueeze(1) # [BATCH, 1, MAX_LENGTH, EM_SIZE]\n",
    "        inputs = [F.tanh(conv(inputs)).squeeze(3) for conv in self.convs] # [BATCH, K_DIM, MAX_LENGTH]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] # [BATCH, K_DIM]*len(Ks)\n",
    "        inputs = torch.cat(inputs, 1) # [BATCH, K_DIM*len(Ks)]\n",
    "        if is_training:\n",
    "            inputs = self.dropout(inputs)\n",
    "        for layer in range(self.hw_num_layers):\n",
    "            gate = F.sigmoid(self.hw_gate[layer](inputs))\n",
    "            nonlinear = F.relu(self.hw_nonlinear[layer](inputs))\n",
    "            linear = self.hw_linear[layer](inputs)\n",
    "            inputs = gate * nonlinear + (1 - gate) * linear\n",
    "        if is_training:\n",
    "            inputs = self.dropout(inputs)\n",
    "        inputs = inputs.view(-1, inputs.size()[1]*2)\n",
    "        out = self.final_layer(inputs)\n",
    "        return F.log_softmax(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 15\n",
    "KERNEL_SIZES = [1, 2, 3, 4, 5, 6, 7]\n",
    "KERNEL_DIMEN = [50, 100, 150, 200, 200, 200, 200]\n",
    "LR = 0.001\n",
    "vocab_size = len(char_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2CNN(vocab_size, EMBEDDING_SIZE, KERNEL_DIMEN, KERNEL_SIZES)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] mean_loss : 0.70\n",
      "[0/5] mean_loss : 0.64\n",
      "[0/5] mean_loss : 0.53\n",
      "[0/5] mean_loss : 0.55\n",
      "[0/5] mean_loss : 0.55\n",
      "[0/5] mean_loss : 0.57\n",
      "[0/5] mean_loss : 0.48\n",
      "[0/5] mean_loss : 0.53\n",
      "[1/5] mean_loss : 0.65\n",
      "[1/5] mean_loss : 0.56\n",
      "[1/5] mean_loss : 0.50\n",
      "[1/5] mean_loss : 0.52\n",
      "[1/5] mean_loss : 0.53\n",
      "[1/5] mean_loss : 0.56\n",
      "[1/5] mean_loss : 0.47\n",
      "[1/5] mean_loss : 0.52\n",
      "[2/5] mean_loss : 0.72\n",
      "[2/5] mean_loss : 0.55\n",
      "[2/5] mean_loss : 0.49\n",
      "[2/5] mean_loss : 0.52\n",
      "[2/5] mean_loss : 0.52\n",
      "[2/5] mean_loss : 0.55\n",
      "[2/5] mean_loss : 0.47\n",
      "[2/5] mean_loss : 0.51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-8932e03052ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stocknlp\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stocknlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mgrad_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mgrad_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_variables\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\stocknlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, user_create_graph)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    for i, batch in enumerate(get_batch(filenames, BUFFER_SIZE, BATCH_SIZE)):\n",
    "        inputs, targets = pad_to_batch(batch)\n",
    "        model.zero_grad()\n",
    "        preds = model(inputs, True)\n",
    "        loss = loss_function(preds, targets)\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
