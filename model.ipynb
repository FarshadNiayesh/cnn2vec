{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import math\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "WINDOW = 3\n",
    "K = 5\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 15\n",
    "KERNEL_SIZES = [1, 2, 3, 4, 5, 6, 7]\n",
    "KERNEL_DIMEN = [50, 100, 150, 200, 200, 200, 200]\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 1975658), ('the', 1417475), ('of', 888701), ('\\uf8ff', 781685), ('and', 726249)]\n"
     ]
    }
   ],
   "source": [
    "word_vocab = Counter()\n",
    "char_vocab = Counter()\n",
    "char_vocab.update(['{', '}'])\n",
    "text_location = os.path.join(os.getcwd(), 'corpus/')\n",
    "filenames = os.listdir(text_location)\n",
    "for filename in filenames:\n",
    "    filename = os.path.join(text_location, filename)\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        line = f.read()\n",
    "        word_vocab.update(line.lower().split())\n",
    "        char_vocab.update(line)\n",
    "print(word_vocab.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +1 as 0 is the PAD\n",
    "char_to_index = {e:n+1 for n, e in enumerate(char_vocab)}\n",
    "index_to_char = {n+1:e for n, e in enumerate(char_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_words = sum([num for word, num in word_vocab.items()])\n",
    "unigram_table = []\n",
    "Z = 0.001\n",
    "for word in word_vocab:\n",
    "    unigram_table.extend([word] * int(((word_vocab[word]/num_total_words)**0.75)/Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative(word):\n",
    "    neg_samples = []\n",
    "    word = \"\".join([index_to_char[ind] for ind in word])\n",
    "    while len(neg_samples) < K:\n",
    "        neg = random.choice(unigram_table)\n",
    "        if neg == word.lower():\n",
    "            continue\n",
    "        neg_samples.append(prepare_word(neg, char_to_index))\n",
    "    return neg_samples\n",
    "\n",
    "def prepare_files(filenames):\n",
    "    MIN_COUNT = 2\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                max_j = len(words)\n",
    "                for i, word in enumerate(words):\n",
    "                    if word_vocab[word.lower()] <= MIN_COUNT:\n",
    "                        continue\n",
    "                    frequency = word_vocab[word.lower()] / num_total_words\n",
    "                    number = 1 - math.sqrt(0.00005/frequency)\n",
    "                    if random.uniform(0, 1) <= number:\n",
    "                        continue\n",
    "                    for j in range(i - WINDOW, i + WINDOW):\n",
    "                        if (i == j) or (j < 0) or (j >= max_j):\n",
    "                            continue\n",
    "                        target = words[j]\n",
    "                        yield word, target\n",
    "\n",
    "def prepare_word(word, char_to_index):\n",
    "    start = [char_to_index['{']]\n",
    "    finish = [char_to_index['}']]\n",
    "    return start + [char_to_index[char] for char in word] + finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buffer(filenames, buffer_size):\n",
    "    random.shuffle(filenames)\n",
    "    buffer = []\n",
    "    for word, target in prepare_files(filenames):\n",
    "        word = prepare_word(word, char_to_index)\n",
    "        target = prepare_word(target, char_to_index)\n",
    "        buffer.append([word, target])\n",
    "        if len(buffer) == buffer_size:\n",
    "            yield buffer\n",
    "            buffer = []\n",
    "    yield buffer\n",
    "    \n",
    "def get_batch(filenames, buffer_size, batch_size):\n",
    "    for buffer in get_buffer(filenames, buffer_size):\n",
    "        random.shuffle(buffer)\n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < len(buffer):\n",
    "            batch = buffer[sindex:eindex]\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            yield batch\n",
    "        if eindex >= len(buffer):\n",
    "            batch = buffer[sindex:]\n",
    "            yield batch\n",
    "            \n",
    "def pad_to_batch(batch):\n",
    "    max_length = max([len(e) for e in batch])\n",
    "    padded_batch = []\n",
    "    for i in range(len(batch)):\n",
    "        padded_batch.append(batch[i] + [0] * (max_length - len(batch[i])))\n",
    "    return Variable(LongTensor(padded_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_location = os.path.join(os.getcwd(), 'corpus/')\n",
    "filenames = [os.path.join(text_location, filename) for filename in os.listdir(text_location)]\n",
    "batches = get_batch(filenames, BUFFER_SIZE, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, kernel_dims, kernel_sizes,\n",
    "                 highway_layers=2):\n",
    "        super(Word2CNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim, (size, embedding_dim)) for dim, size in zip(kernel_dims, kernel_sizes)])\n",
    "        self.internal_dim = sum(kernel_dims)\n",
    "        self.hw_num_layers = highway_layers\n",
    "        self.hw_nonlinear = nn.ModuleList([nn.Linear(self.internal_dim, self.internal_dim) for _ in range(highway_layers)])\n",
    "        self.hw_linear = nn.ModuleList([nn.Linear(self.internal_dim, self.internal_dim) for _ in range(highway_layers)])\n",
    "        self.hw_gate = nn.ModuleList([nn.Linear(self.internal_dim, self.internal_dim) for _ in range(highway_layers)])\n",
    "        self.final_layer = nn.Linear(self.internal_dim * 2, 2)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "        \n",
    "    def char_cnn(self, inputs):\n",
    "        inputs = self.embeddings(inputs).unsqueeze(1) # [BATCH, 1, MAX_LENGTH, EM_SIZE]\n",
    "        inputs = [F.tanh(conv(inputs)).squeeze(3) for conv in self.convs] # [BATCH, K_DIM, MAX_LENGTH]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] # [BATCH, K_DIM]*len(Ks)\n",
    "        inputs = torch.cat(inputs, 1) # [BATCH, K_DIM*len(Ks)]\n",
    "        for layer in range(self.hw_num_layers):\n",
    "            gate = F.sigmoid(self.hw_gate[layer](inputs))\n",
    "            nonlinear = F.relu(self.hw_nonlinear[layer](inputs))\n",
    "            linear = self.hw_linear[layer](inputs)\n",
    "            inputs = gate * nonlinear + (1 - gate) * linear\n",
    "        return inputs\n",
    "    \n",
    "    def forward(self, center_words, target_words, negati_words, is_training=False):      \n",
    "        center_embeds = self.char_cnn(center_words).unsqueeze(1) # [B, 1, D]\n",
    "        target_embeds = self.char_cnn(target_words).unsqueeze(1) # [B, 1, D]\n",
    "        size = negati_words.size()\n",
    "        batch_size = size[0]\n",
    "        K = size[1]\n",
    "        wlen = size[2]\n",
    "        negati_words = negati_words.view(batch_size*K, wlen)\n",
    "        negati_embeds = -self.char_cnn(negati_words).view(batch_size, K, self.internal_dim) # [B, K, D]\n",
    "        \n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1\n",
    "        negative_score = torch.sum(negati_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2), 1).view(batch_size, -1)\n",
    "        loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        return self.char_cnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_index) + 1\n",
    "model = Word2CNN(vocab_size, EMBEDDING_SIZE, KERNEL_DIMEN, KERNEL_SIZES)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] mean_loss : 29.49\n",
      "[0/5] mean_loss : 1.59\n",
      "[0/5] mean_loss : 1.29\n",
      "[0/5] mean_loss : 1.20\n",
      "[0/5] mean_loss : 1.09\n",
      "[0/5] mean_loss : 0.90\n",
      "[0/5] mean_loss : 1.07\n",
      "[0/5] mean_loss : 1.13\n",
      "[0/5] mean_loss : 1.06\n",
      "[0/5] mean_loss : 1.07\n",
      "[0/5] mean_loss : 1.06\n",
      "[0/5] mean_loss : 1.00\n",
      "[0/5] mean_loss : 0.98\n",
      "[0/5] mean_loss : 0.84\n",
      "[0/5] mean_loss : 0.41\n",
      "[0/5] mean_loss : 0.75\n",
      "[0/5] mean_loss : 0.95\n",
      "[0/5] mean_loss : 0.85\n",
      "[0/5] mean_loss : 0.81\n",
      "[0/5] mean_loss : 0.73\n",
      "[0/5] mean_loss : 0.77\n",
      "[0/5] mean_loss : 0.67\n",
      "[0/5] mean_loss : 0.82\n",
      "[0/5] mean_loss : 0.81\n",
      "[0/5] mean_loss : 0.79\n",
      "[0/5] mean_loss : 0.78\n",
      "[0/5] mean_loss : 0.82\n",
      "[0/5] mean_loss : 0.83\n",
      "[0/5] mean_loss : 0.85\n",
      "[0/5] mean_loss : 0.81\n",
      "[0/5] mean_loss : 0.76\n",
      "[0/5] mean_loss : 0.84\n",
      "[0/5] mean_loss : 0.73\n",
      "[0/5] mean_loss : 0.80\n",
      "[0/5] mean_loss : 0.84\n",
      "[0/5] mean_loss : 0.89\n",
      "[0/5] mean_loss : 0.81\n",
      "[0/5] mean_loss : 0.75\n",
      "[0/5] mean_loss : 0.81\n",
      "[0/5] mean_loss : 0.73\n",
      "[0/5] mean_loss : 0.70\n",
      "[0/5] mean_loss : 1.76\n",
      "[0/5] mean_loss : 1.11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-ce2f328837fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_to_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_to_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mnegatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_to_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegatives\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mnegatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnegatives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-266-15143183635f>\u001b[0m in \u001b[0;36mpad_to_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mpadded_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    for i, batch in enumerate(get_batch(filenames, BUFFER_SIZE, BATCH_SIZE)):\n",
    "        inputs, targets = zip(*batch)\n",
    "        negatives = []\n",
    "        for inpt in inputs:\n",
    "            negatives.extend(get_negative(inpt))\n",
    "        inputs = pad_to_batch(inputs)\n",
    "        targets = pad_to_batch(targets)\n",
    "        negatives = pad_to_batch(negatives)\n",
    "        negatives = negatives.view(len(inputs), K, -1)\n",
    "        model.zero_grad()\n",
    "        loss = model(inputs, targets, negatives, True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        if i % 100 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"the_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
